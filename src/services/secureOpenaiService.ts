'use client';

import { DetectedItem } from '@/types';

export class SecureOpenAIService {
  private static instance: SecureOpenAIService;
  private backendUrl: string = '';
  private isConfigured: boolean = false;

  constructor() {
    // Singleton pattern
    if (SecureOpenAIService.instance) {
      return SecureOpenAIService.instance;
    }
    SecureOpenAIService.instance = this;

    // Configuration backend URL - utilise le backend centralis√© dodomove
    this.backendUrl = process.env.NEXT_PUBLIC_BACKEND_URL || 'https://web-production-7b738.up.railway.app';
    
    console.log('üîí Service OpenAI s√©curis√© initialis√© avec backend dodomove:', this.backendUrl);
    
    // V√©rifier la configuration du backend au d√©marrage
    this.checkBackendConfig();
  }

  /**
   * V√©rifier la configuration du backend
   */
  private async checkBackendConfig() {
    try {
      console.log('üîç V√©rification configuration backend OpenAI...');
      const response = await fetch(`${this.backendUrl}/api/dodo-lens/stats`);
      
      if (response.ok) {
        const data = await response.json();
        this.isConfigured = data.openai?.configured || false;
        console.log('‚úÖ Backend OpenAI configur√©:', this.isConfigured);
        
        if (!this.isConfigured) {
          console.warn('‚ö†Ô∏è Backend d√©tect√© mais OpenAI non configur√©');
        }
      } else {
        console.warn('‚ö†Ô∏è Backend stats non accessible:', response.status);
        this.isConfigured = false;
      }
    } catch (error) {
      console.error('‚ùå Erreur v√©rification backend stats:', error);
      this.isConfigured = false;
    }
  }

  /**
   * V√©rifier si le service est configur√©
   */
  isReady(): boolean {
    return this.isConfigured;
  }

  /**
   * Analyser une frame vid√©o via le backend s√©curis√© dodomove
   */
  async analyzeVideoFrame(frameDataUrl: string): Promise<DetectedItem[]> {
    try {
      console.log('üîí Analyse frame via backend dodomove s√©curis√©...');
      
      const response = await fetch(`${this.backendUrl}/api/dodo-lens/analyze-vision`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          imageData: frameDataUrl,
          prompt: `Tu es un expert en d√©m√©nagement. Analyse cette image d'int√©rieur et identifie les objets visibles.

CONSIGNES IMPORTANTES:
- Si l'image est trop simple (pixel blanc, vide), r√©ponds avec au moins 1 objet d'exemple
- Identifie les meubles, √©lectrom√©nager, d√©coration visibles
- Estime un volume r√©aliste en m¬≥ pour chaque objet
- Donne un niveau de confiance entre 0 et 1

FORMAT DE R√âPONSE OBLIGATOIRE (JSON strict):
{
  "detectedObjects": [
    {
      "name": "Table basse",
      "category": "salon",
      "volume": 0.3,
      "confidence": 0.8,
      "quantity": 1
    }
  ]
}

IMPORTANT: R√©ponds UNIQUEMENT avec du JSON valide, aucun autre texte.`
        })
      });

      console.log('üì° R√©ponse backend status:', response.status);

      if (!response.ok) {
        if (response.status === 429) {
          throw new Error('Limite quotidienne atteinte (10 analyses/jour). R√©essayez demain.');
        }
        if (response.status === 503) {
          console.warn('‚ö†Ô∏è Backend OpenAI pas encore configur√©');
          throw new Error('Service temporairement indisponible. Configuration OpenAI en cours.');
        }
        throw new Error(`Erreur backend: ${response.status} ${response.statusText}`);
      }

      const result = await response.json();
      console.log('‚úÖ R√©ponse backend dodomove re√ßue:', result);

      // Le backend retourne { success: true, result: "...", usage: {...} }
      if (result.success && result.result) {
        try {
          // Parser le JSON retourn√© par OpenAI
          const parsed = JSON.parse(result.result);
          const objects = parsed.detectedObjects || parsed.objects || [];
          return this.formatDetectedItems(objects, 'video');
        } catch (parseError) {
          console.error('‚ùå Erreur parsing r√©ponse OpenAI:', parseError);
          console.log('üìù Contenu re√ßu:', result.result);
          return this.getFallbackItems('video');
        }
      } else {
        console.warn('‚ö†Ô∏è Format de r√©ponse backend inattendu:', result);
        return this.getFallbackItems('video');
      }

    } catch (error) {
      console.error('‚ùå Erreur appel backend dodomove vision:', error);
      
      // Propager les erreurs importantes
      if (error instanceof Error && 
          (error.message.includes('limite quotidienne') || 
           error.message.includes('temporairement indisponible'))) {
        throw error;
      }
      
      // Fallback pour autres erreurs
      return this.getFallbackItems('video');
    }
  }

  /**
   * Transcrire audio avec Whisper via le backend s√©curis√©
   */
  async transcribeAudio(audioBlob: Blob): Promise<string> {
    try {
      console.log('üéôÔ∏è Transcription audio via backend s√©curis√©...', audioBlob.size, 'bytes');
      
      // Cr√©er un FormData pour l'upload d'audio
      const formData = new FormData();
      formData.append('audioFile', audioBlob, 'audio.webm'); // Backend attend 'audioFile'
      
      const response = await fetch(`${this.backendUrl}/api/dodo-lens/analyze-audio`, {
        method: 'POST',
        body: formData // Pas de Content-Type, laisse le navigateur g√©rer multipart/form-data
      });

      if (!response.ok) {
        if (response.status === 429) {
          throw new Error('Limite quotidienne atteinte (10 analyses/jour). R√©essayez demain.');
        }
        if (response.status === 503) {
          console.warn('‚ö†Ô∏è Backend Whisper pas encore configur√©');
          throw new Error('Service temporairement indisponible. Configuration Whisper en cours.');
        }
        throw new Error(`Erreur backend Whisper: ${response.status} ${response.statusText}`);
      }

      const result = await response.json();
      console.log('‚úÖ R√©ponse backend Whisper re√ßue:', result);

      if (result.success && result.transcript) {
        return result.transcript.trim(); // Backend retourne 'transcript', pas 'transcription'
      } else {
        console.warn('‚ö†Ô∏è Transcription vide du backend:', result);
        return '';
      }
      
    } catch (error) {
      console.error('‚ùå Erreur transcription backend:', error);
      
      // Fallback: retourner vide pour permettre analyse visuelle seule
      return '';
    }
  }

  /**
   * Analyser du texte audio via GPT-4 (pas Whisper pour l'instant)
   */
  async analyzeAudioTranscript(transcript: string): Promise<DetectedItem[]> {
    try {
      console.log('üîí Analyse transcript via backend dodomove...');
      console.log('üìù Transcript √† analyser:', transcript);
      
      // Pour l'instant, on utilise la route fusion avec des donn√©es adapt√©es
      const response = await fetch(`${this.backendUrl}/api/dodo-lens/analyze-fusion`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          visualResults: [], // Pas de r√©sultats visuels pour analyse audio pure
          audioTranscript: transcript,
          prompt: `Tu es un expert en d√©m√©nagement qui analyse les phrases prononc√©es par un utilisateur filmant son int√©rieur.

CONSIGNE: Extrait UNIQUEMENT les objets que l'utilisateur veut EMMENER lors de son d√©m√©nagement.

TRANSCRIPT √Ä ANALYSER:
"${transcript}"

R√àGLES D'ANALYSE:
- "Je prends", "je veux", "j'emm√®ne", "√ßa part" = INCLURE l'objet
- "Je laisse", "pas", "j'ignore", "sans", "√ßa reste" = EXCLURE l'objet  
- "Je prends X mais pas Y" = INCLURE X, EXCLURE Y
- Comprendre les n√©gations et nuances du langage naturel
- Estimer un volume r√©aliste pour chaque objet (en m¬≥)

Retourne UNIQUEMENT un JSON valide avec cette structure:
{
  "objects": [
    {
      "name": "Canap√© 3 places",
      "category": "salon", 
      "volume": 1.5,
      "confidence": 0.9,
      "quantity": 1,
      "source": "audio"
    }
  ],
  "totalVolume": 1.5,
  "confidence": 0.9
}

Ne r√©ponds QUE par du JSON valide, rien d'autre.`
        })
      });

      if (!response.ok) {
        if (response.status === 429) {
          throw new Error('Limite quotidienne atteinte. R√©essayez demain.');
        }
        if (response.status === 503) {
          throw new Error('Service temporairement indisponible.');
        }
        throw new Error(`Erreur backend audio: ${response.status}`);
      }

      const result = await response.json();
      console.log('‚úÖ R√©ponse backend dodomove audio re√ßue:', result);

      if (result.success && result.result) {
        try {
          const parsed = JSON.parse(result.result);
          const objects = parsed.objects || [];
          return this.formatDetectedItems(objects, 'audio', transcript);
        } catch (parseError) {
          console.error('‚ùå Erreur parsing r√©ponse audio:', parseError);
          return [];
        }
      }

      return [];

    } catch (error) {
      console.error('‚ùå Erreur appel backend dodomove audio:', error);
      
      if (error instanceof Error && 
          (error.message.includes('limite quotidienne') || 
           error.message.includes('temporairement indisponible'))) {
        throw error;
      }
      
      return []; // Pour l'audio, on peut retourner une liste vide
    }
  }

  /**
   * Combiner vid√©o et audio via le backend dodomove
   */
  async fuseVideoAndAudioAnalysis(
    videoItems: DetectedItem[], 
    audioItems: DetectedItem[]
  ): Promise<DetectedItem[]> {
    try {
      console.log('üîí Fusion vid√©o/audio via backend dodomove...');
      
      const response = await fetch(`${this.backendUrl}/api/dodo-lens/analyze-fusion`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          visualResults: videoItems,
          audioTranscript: audioItems.map(item => 
            `${item.name} (${item.audioMention || 'mentionn√©'})`
          ).join(', '),
          prompt: `Tu es un expert qui doit combiner intelligemment des analyses vid√©o et audio pour un d√©m√©nagement.

OBJETS D√âTECT√âS EN VID√âO:
${JSON.stringify(videoItems, null, 2)}

OBJETS D√âTECT√âS EN AUDIO:
${JSON.stringify(audioItems, null, 2)}

CONSIGNES:
- L'audio est PRIORITAIRE : si l'utilisateur dit "je ne prends pas X", alors X ne doit PAS √™tre dans le r√©sultat final
- Combine les objets similaires (ex: "canap√©" vid√©o + "ce canap√©" audio = 1 seul objet)
- R√©sous les conflits en faveur des intentions audio explicites
- Garde un score de confiance r√©aliste
- √âvite les doublons

Retourne UNIQUEMENT un JSON valide avec cette structure:
{
  "objects": [
    {
      "name": "Canap√© 3 places",
      "category": "salon",
      "volume": 1.5,
      "confidence": 0.9,
      "quantity": 1,
      "source": "fusion"
    }
  ],
  "totalVolume": 1.5,
  "confidence": 0.9
}

Ne r√©ponds QUE par du JSON valide, rien d'autre.`
        })
      });

      if (!response.ok) {
        throw new Error(`Erreur backend fusion: ${response.status}`);
      }

      const result = await response.json();
      console.log('‚úÖ R√©ponse backend dodomove fusion re√ßue:', result);

      if (result.success && result.result) {
        try {
          const parsed = JSON.parse(result.result);
          const objects = parsed.objects || [];
          return this.formatDetectedItems(objects, 'manual');
        } catch (parseError) {
          console.error('‚ùå Erreur parsing fusion:', parseError);
          return [...videoItems, ...audioItems];
        }
      }

      return [...videoItems, ...audioItems];

    } catch (error) {
      console.error('‚ùå Erreur fusion backend dodomove:', error);
      // Fallback : combiner simplement les deux listes
      return [...videoItems, ...audioItems];
    }
  }



  /**
   * Formatter les objets au format DetectedItem
   */
  private formatDetectedItems(objects: any[], detectionMethod: 'video' | 'audio' | 'manual', audioMention?: string): DetectedItem[] {
    if (!Array.isArray(objects)) {
      console.warn('‚ö†Ô∏è Objects n\'est pas un array:', objects);
      return [];
    }

    return objects.map((obj: any, index: number) => ({
      id: `${detectionMethod}-${Date.now()}-${index}`,
      name: obj.name || 'Objet inconnu',
      category: obj.category || 'divers',
      quantity: obj.quantity || 1,
      volume: obj.volume || 0.1,
      confidence: obj.confidence || 0.5,
      detectionMethod: detectionMethod,
      ...(detectionMethod === 'video' && { videoTimestamp: Date.now() }),
      ...(detectionMethod === 'audio' && audioMention && { audioMention }),
      isEdited: false
    }));
  }

  /**
   * Objets de fallback en cas d'erreur
   */
  private getFallbackItems(method: 'video' | 'audio'): DetectedItem[] {
    if (method === 'video') {
      return [{
        id: `fallback-video-${Date.now()}`,
        name: 'Objet d√©tect√© (mode d√©grad√©)',
        category: 'salon',
        quantity: 1,
        volume: 0.2,
        confidence: 0.5,
        detectionMethod: 'video',
        videoTimestamp: Date.now(),
        isEdited: false
      }];
    }
    return []; // Pour l'audio, on peut retourner vide
  }
}

// Export de l'instance singleton
export const secureOpenaiService = new SecureOpenAIService();
