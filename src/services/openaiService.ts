'use client';

import { DetectedItem } from '@/types';

export class OpenAIService {
  private static instance: OpenAIService;
  private openai: any = null;
  private isConfigured: boolean = false;
  private isInitializing: boolean = false;

  constructor() {
    // Singleton pattern
    if (OpenAIService.instance) {
      return OpenAIService.instance;
    }
    OpenAIService.instance = this;

    // Initialiser OpenAI uniquement c√¥t√© client
    if (typeof window !== 'undefined') {
      this.initializeOpenAI();
    }
  }

  private async initializeOpenAI() {
    if (this.isInitializing) return;
    this.isInitializing = true;

    console.log('üîÑ Initialisation du service OpenAI...');

    const apiKey = process.env.NEXT_PUBLIC_OPENAI_API_KEY;
    
    if (!apiKey || apiKey === 'sk-your-openai-api-key-here') {
      console.warn('‚ö†Ô∏è Cl√© API OpenAI manquante ou non configur√©e');
      this.isConfigured = false;
      this.isInitializing = false;
      return;
    }

    try {
      // Import dynamique du module OpenAI
      console.log('üì¶ Chargement du module OpenAI...');
      const OpenAIModule = await import('openai');
      const OpenAI = OpenAIModule.default;

      console.log('üîë Configuration du client OpenAI...');
      this.openai = new OpenAI({
        apiKey: apiKey,
        dangerouslyAllowBrowser: true // N√©cessaire pour le client-side
      });
      
      this.isConfigured = true;
      console.log('‚úÖ Service OpenAI initialis√© avec succ√®s');
    } catch (error) {
      console.error('‚ùå Erreur initialisation OpenAI:', error);
      this.isConfigured = false;
    } finally {
      this.isInitializing = false;
    }
  }

  /**
   * V√©rifier si le service est configur√©
   */
  isReady(): boolean {
    return this.isConfigured;
  }

  /**
   * Analyser une frame vid√©o avec GPT-4 Vision
   */
  async analyzeVideoFrame(frameDataUrl: string): Promise<DetectedItem[]> {
    if (!this.isConfigured) {
      throw new Error('Service OpenAI non configur√©');
    }

    try {
      console.log('üîÑ Analyse frame avec GPT-4o (vision int√©gr√©e)...');
      
      const response = await this.openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          {
            role: "user",
            content: [
              {
                type: "text",
                text: `Tu es un expert en d√©m√©nagement. Analyse cette image d'int√©rieur et identifie les objets visibles.

CONSIGNES IMPORTANTES:
- Si l'image est trop simple (pixel blanc, vide), r√©ponds avec au moins 1 objet d'exemple
- Identifie les meubles, √©lectrom√©nager, d√©coration visibles
- Estime un volume r√©aliste en m¬≥ pour chaque objet
- Donne un niveau de confiance entre 0 et 1

FORMAT DE R√âPONSE OBLIGATOIRE (JSON strict):
{
  "detectedObjects": [
    {
      "name": "Table basse",
      "category": "salon",
      "volume": 0.3,
      "confidence": 0.8,
      "quantity": 1
    }
  ]
}

IMPORTANT: R√©ponds UNIQUEMENT avec du JSON valide, aucun autre texte.`
              },
              {
                type: "image_url",
                image_url: {
                  url: frameDataUrl
                }
              }
            ]
          }
        ],
        max_tokens: 1500,
        temperature: 0.1
      });

      console.log('üì¶ R√©ponse brute OpenAI:', response);
      
      const content = response.choices[0]?.message?.content;
      console.log('üìù Contenu de la r√©ponse:', content);
      
      if (!content) {
        console.error('‚ùå Aucun contenu dans la r√©ponse:', response);
        throw new Error('R√©ponse vide de OpenAI - aucun contenu retourn√©');
      }

      if (content.trim() === '') {
        console.error('‚ùå Contenu vide dans la r√©ponse');
        throw new Error('R√©ponse vide de OpenAI - contenu vide');
      }

      // Parser la r√©ponse JSON avec gestion d'erreur robuste
      let parsed;
      try {
        parsed = JSON.parse(content);
        console.log('‚úÖ JSON pars√© avec succ√®s:', parsed);
      } catch (parseError) {
        console.error('‚ùå Erreur parsing JSON:', parseError);
        console.error('üìù Contenu re√ßu:', content);
        
        // Essayer de nettoyer le JSON (supprimer backticks, etc.)
        const cleanContent = content.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
        console.log('üßπ Contenu nettoy√©:', cleanContent);
        
        try {
          parsed = JSON.parse(cleanContent);
          console.log('‚úÖ JSON nettoy√© pars√© avec succ√®s:', parsed);
        } catch (secondParseError) {
          console.error('‚ùå √âchec parsing JSON m√™me apr√®s nettoyage:', secondParseError);
          // Retourner un objet d'exemple en cas d'√©chec total
          return [{
            id: `fallback-${Date.now()}`,
            name: 'Objet d√©tect√© (fallback)',
            category: 'divers',
            quantity: 1,
            volume: 0.1,
            confidence: 0.5,
            detectionMethod: 'video' as const,
            videoTimestamp: Date.now(),
            isEdited: false
          }];
        }
      }

      const objects = parsed.detectedObjects || parsed.objects || [];
      console.log('üìã Objets extraits:', objects);

      if (!Array.isArray(objects)) {
        console.warn('‚ö†Ô∏è Les objets ne sont pas un array, conversion...');
        return [{
          id: `single-${Date.now()}`,
          name: 'Objet unique d√©tect√©',
          category: 'divers',
          quantity: 1,
          volume: 0.1,
          confidence: 0.5,
          detectionMethod: 'video' as const,
          videoTimestamp: Date.now(),
          isEdited: false
        }];
      }

      if (objects.length === 0) {
        console.warn('‚ö†Ô∏è Aucun objet d√©tect√©, retour objet d\'exemple...');
        return [{
          id: `example-${Date.now()}`,
          name: 'Objet d\'exemple',
          category: 'salon',
          quantity: 1,
          volume: 0.2,
          confidence: 0.6,
          detectionMethod: 'video' as const,
          videoTimestamp: Date.now(),
          isEdited: false
        }];
      }

      // Convertir au format DetectedItem
      return objects.map((obj: any, index: number) => ({
        id: `video-${Date.now()}-${index}`,
        name: obj.name || 'Objet inconnu',
        category: obj.category || 'divers',
        quantity: obj.quantity || 1,
        volume: obj.volume || 0.1,
        confidence: obj.confidence || 0.5,
        detectionMethod: 'video' as const,
        videoTimestamp: Date.now(),
        isEdited: false
      }));

    } catch (error) {
      console.error('‚ùå Erreur analyse GPT-4 Vision:', error);
      throw error;
    }
  }

  /**
   * Analyser du texte audio avec GPT-4
   */
  async analyzeAudioTranscript(transcript: string): Promise<DetectedItem[]> {
    if (!this.isConfigured) {
      throw new Error('Service OpenAI non configur√©');
    }

    try {
      console.log('üîÑ Analyse transcript avec GPT-4...');
      console.log('üìù Transcript re√ßu:', `"${transcript}"`);
      console.log('üìè Longueur:', transcript.length, 'caract√®res');
      
      const response = await this.openai.chat.completions.create({
        model: "gpt-4",
        messages: [
          {
            role: "user",
            content: `Tu es un expert en d√©m√©nagement qui analyse les phrases prononc√©es par un utilisateur filmant son int√©rieur.

CONSIGNE: Extrait UNIQUEMENT les objets que l'utilisateur veut EMMENER lors de son d√©m√©nagement.

TRANSCRIPT √Ä ANALYSER:
"${transcript}"

R√àGLES D'ANALYSE:
- "Je prends", "je veux", "j'emm√®ne", "√ßa part" = INCLURE l'objet
- "Je laisse", "pas", "j'ignore", "sans", "√ßa reste" = EXCLURE l'objet  
- "Je prends X mais pas Y" = INCLURE X, EXCLURE Y
- Comprendre les n√©gations et nuances du langage naturel
- Estimer un volume r√©aliste pour chaque objet (en m¬≥)

R√âPONSE ATTENDUE (JSON uniquement):
{
  "detectedObjects": [
    {
      "name": "Canap√© 3 places",
      "category": "salon", 
      "volume": 1.5,
      "confidence": 0.9,
      "quantity": 1,
      "reasoning": "Utilisateur dit clairement qu'il l'emm√®ne"
    }
  ]
}

Ne r√©ponds QUE par du JSON valide, rien d'autre.`
          }
        ],
        max_tokens: 1000,
        temperature: 0.1
      });

      console.log('üì¶ R√©ponse brute GPT-4 Audio:', response);
      
      const content = response.choices[0]?.message?.content;
      console.log('üìù Contenu extrait:', content);
      
      if (!content) {
        throw new Error('R√©ponse vide de OpenAI');
      }

      // Parser la r√©ponse JSON
      const parsed = JSON.parse(content);
      console.log('üîß JSON pars√©:', parsed);
      
      const objects = parsed.detectedObjects || [];
      console.log('üìã Objets extraits:', objects);
      console.log('üìä Nombre d\'objets:', objects.length);

      if (objects.length === 0) {
        console.warn('‚ö†Ô∏è GPT-4 Audio a retourn√© une liste vide d\'objets !');
        console.warn('üìù Pour le transcript:', transcript);
      }

      // Convertir au format DetectedItem
      const result = objects.map((obj: any, index: number) => ({
        id: `audio-${Date.now()}-${index}`,
        name: obj.name || 'Objet inconnu',
        category: obj.category || 'divers',
        quantity: obj.quantity || 1,
        volume: obj.volume || 0.1,
        confidence: obj.confidence || 0.5,
        detectionMethod: 'audio' as const,
        audioMention: transcript,
        isEdited: false
      }));
      
      console.log('‚úÖ R√©sultat final GPT-4 Audio:', result);
      return result;

    } catch (error) {
      console.error('‚ùå Erreur analyse GPT-4 Audio:', error);
      throw error;
    }
  }



  /**
   * Transcrire un fichier audio avec Whisper API
   */
  async transcribeAudio(audioBlob: Blob): Promise<string> {
    if (!this.openai) {
      console.warn('OpenAI non initialis√© pour Whisper');
      return '';
    }

    try {
      console.log('üéôÔ∏è Envoi audio √† Whisper API...', audioBlob.size, 'bytes');
      
      // Cr√©er un fichier FormData pour Whisper
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.webm');
      formData.append('model', 'whisper-1');
      formData.append('language', 'fr');
      formData.append('response_format', 'text');

      // Appel direct √† l'API Whisper
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.NEXT_PUBLIC_OPENAI_API_KEY}`,
        },
        body: formData
      });

      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.status}`);
      }

      const transcription = await response.text();
      console.log('‚úÖ Transcription Whisper re√ßue:', transcription);
      
      return transcription.trim();
      
    } catch (error) {
      console.error('‚ùå Erreur Whisper API:', error);
      return '';
    }
  }

  /**
   * Combiner et r√©soudre les conflits entre vid√©o et audio
   */
  async fuseVideoAndAudioAnalysis(
    videoItems: DetectedItem[], 
    audioItems: DetectedItem[]
  ): Promise<DetectedItem[]> {
    if (!this.isConfigured) {
      throw new Error('Service OpenAI non configur√©');
    }

    try {
      console.log('üîÑ Fusion intelligente vid√©o + audio...');
      
      const response = await this.openai.chat.completions.create({
        model: "gpt-4",
        messages: [
          {
            role: "user",
            content: `Tu es un expert qui doit combiner intelligemment des analyses vid√©o et audio pour un d√©m√©nagement.

OBJETS D√âTECT√âS EN VID√âO:
${JSON.stringify(videoItems, null, 2)}

OBJETS D√âTECT√âS EN AUDIO:
${JSON.stringify(audioItems, null, 2)}

CONSIGNES:
- L'audio est PRIORITAIRE : si l'utilisateur dit "je ne prends pas X", alors X ne doit PAS √™tre dans le r√©sultat final
- Combine les objets similaires (ex: "canap√©" vid√©o + "ce canap√©" audio = 1 seul objet)
- R√©sous les conflits en faveur des intentions audio explicites
- Garde un score de confiance r√©aliste
- √âvite les doublons

R√âPONSE ATTENDUE (JSON uniquement):
{
  "fusedObjects": [
    {
      "name": "Canap√© 3 places",
      "category": "salon",
      "volume": 1.5,
      "confidence": 0.9,
      "quantity": 1,
      "sources": ["video", "audio"],
      "reasoning": "D√©tect√© en vid√©o et confirm√© en audio"
    }
  ]
}

Ne r√©ponds QUE par du JSON valide, rien d'autre.`
          }
        ],
        max_tokens: 2000,
        temperature: 0.1
      });

      const content = response.choices[0]?.message?.content;
      if (!content) {
        throw new Error('R√©ponse vide de OpenAI');
      }

      // Parser la r√©ponse JSON
      const parsed = JSON.parse(content);
      const objects = parsed.fusedObjects || [];

      // Convertir au format DetectedItem
      return objects.map((obj: any, index: number) => ({
        id: `fused-${Date.now()}-${index}`,
        name: obj.name || 'Objet inconnu',
        category: obj.category || 'divers',
        quantity: obj.quantity || 1,
        volume: obj.volume || 0.1,
        confidence: obj.confidence || 0.5,
        detectionMethod: 'manual' as const, // Fusion = manuel
        isEdited: false
      }));

    } catch (error) {
      console.error('‚ùå Erreur fusion IA:', error);
      // En cas d'erreur, retourner la combinaison simple
      return [...videoItems, ...audioItems];
    }
  }
}

// Export de l'instance singleton
export const openaiService = new OpenAIService();
