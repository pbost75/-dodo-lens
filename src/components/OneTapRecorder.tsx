'use client';

import React, { useState, useRef, useEffect } from 'react';
import { mobileLog } from './MobileDebugPanel';
import { openaiService } from '../services/openaiService';

interface Props {
  onRecordingComplete: (videoBlob: Blob, phrases: string[], audioBlob?: Blob) => void;
}

export const OneTapRecorder: React.FC<Props> = ({ onRecordingComplete }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [hasPermissions, setHasPermissions] = useState<boolean | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [recordingTime, setRecordingTime] = useState(0);
  const [isListening, setIsListening] = useState(false);
  const [capturedPhrases, setCapturedPhrases] = useState<string[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [audioLevel, setAudioLevel] = useState(0);
  const [supportedMimeTypes, setSupportedMimeTypes] = useState<string[]>([]);
  const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
  const [hasAudioInVideo, setHasAudioInVideo] = useState(false);

  const videoRef = useRef<HTMLVideoElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const recognitionRef = useRef<any>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const audioRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);

  // D√©tection mobile
  const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);

  // D√©tecter les formats support√©s au chargement
  useEffect(() => {
    const types = [
      'video/webm;codecs=vp9,opus',
      'video/webm;codecs=vp8,opus', 
      'video/webm;codecs=h264,opus',
      'video/webm',
      'video/mp4;codecs=h264,aac',
      'video/mp4',
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/wav'
    ];
    
    const supported = types.filter(type => MediaRecorder.isTypeSupported(type));
    setSupportedMimeTypes(supported);
  }, []);

  // Cleanup au unmount
  useEffect(() => {
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  // Demander les permissions S√âPAR√âMENT (SOLUTION RADICALE)
  const requestPermissions = async () => {
    try {
      setError(null);
      mobileLog.info('üîê SOLUTION RADICALE : Permissions s√©par√©es');
      
      // 1. VID√âO D'ABORD (SANS AUDIO pour √©viter les conflits Speech Recognition)
      mobileLog.info('üìπ Demande permission VID√âO seulement...');
      const videoStream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: isMobile ? 'environment' : 'user',
          width: { ideal: isMobile ? 640 : 1280 },
          height: { ideal: isMobile ? 480 : 720 }
        },
        audio: false // PAS D'AUDIO !
      });
      mobileLog.info('‚úÖ Stream vid√©o obtenu');

      // 2. AUDIO S√âPAR√âMENT (pour l'enregistrement, Speech Recognition aura son propre acc√®s)
      mobileLog.info('üéôÔ∏è Demande permission AUDIO s√©par√©ment...');
      const audioStream = await navigator.mediaDevices.getUserMedia({
        video: false,
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100,
          channelCount: 1
        }
      });
      mobileLog.info('‚úÖ Stream audio obtenu s√©par√©ment');

      // 3. COMBINER pour l'enregistrement
      const combinedStream = new MediaStream();
      videoStream.getVideoTracks().forEach(track => combinedStream.addTrack(track));
      audioStream.getAudioTracks().forEach(track => combinedStream.addTrack(track));

      streamRef.current = combinedStream; // Stream combin√© pour l'enregistrement
      
      if (videoRef.current) {
        videoRef.current.srcObject = videoStream; // Affichage vid√©o seulement
      }

      // Analyser le niveau audio pour v√©rifier que le micro fonctionne
      try {
        const audioContext = new AudioContext();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(audioStream); // Utiliser le stream audio s√©par√©
        microphone.connect(analyser);
        
        analyser.fftSize = 256;
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        
        const updateAudioLevel = () => {
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
          setAudioLevel(Math.round(average));
        };
        
        // Mettre √† jour le niveau audio toutes les 100ms
        const audioLevelInterval = setInterval(updateAudioLevel, 100);
        
        // Nettoyer l'interval au cleanup  
        // streamRef.current = combinedStream; // D√©j√† fait plus haut
        
        // Ajouter le nettoyage de l'audio context
        const originalTracks = combinedStream.getTracks();
        originalTracks.forEach(track => {
          const originalStop = track.stop;
          track.stop = function() {
            clearInterval(audioLevelInterval);
            audioContext.close();
            originalStop.call(this);
          };
        });
      } catch (audioError) {
        console.warn('Analyse audio non disponible:', audioError);
      }

      setHasPermissions(true);
      return true;
    } catch (err) {
      console.error('Erreur permissions:', err);
      setError('Impossible d\'acc√©der √† la cam√©ra et au microphone. V√©rifiez les permissions.');
      setHasPermissions(false);
      return false;
    }
  };

  // Initialiser la reconnaissance vocale
  const initSpeechRecognition = (): boolean | string => {
    mobileLog.info('üéôÔ∏è D√©but initialisation Speech Recognition');
    
    // Forcer l'utilisation du Web Speech API m√™me sur mobile
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      mobileLog.warn('‚ùå Speech Recognition non disponible sur ce navigateur');
      console.warn('Speech Recognition non disponible');
      return 'fallback'; // On va utiliser un fallback
    }

    mobileLog.info('‚úÖ Speech Recognition disponible');
    console.log('üéôÔ∏è Initialisation Speech Recognition...');
    const recognition = new SpeechRecognition();

    // D√©tection sp√©cifique Android Chrome (le plus probl√©matique)
    const isAndroidChrome = /Android.*Chrome/i.test(navigator.userAgent);
    const isMobile = /Mobi|Android/i.test(navigator.userAgent);
    
    mobileLog.info('üì± D√©tection navigateur:', {
      isAndroidChrome: isAndroidChrome,
      isMobile: isMobile,
      userAgent: navigator.userAgent
    });
    
    if (isAndroidChrome) {
      mobileLog.info('ü§ñ Configuration ANDROID CHROME sp√©ciale');
      console.log('ü§ñ Configuration ANDROID CHROME sp√©ciale');
      // Configuration ultra-sensible pour Android Chrome
      recognition.continuous = true;   // TESTE: true pour capturer plus
      recognition.interimResults = true; // TESTE: true pour plus de sensibilit√©
      recognition.lang = 'fr-FR';
      recognition.maxAlternatives = 3; // Plus d'alternatives
      
      mobileLog.info('‚öôÔ∏è Config Android SENSIBLE: continuous=true, interimResults=true');
      
      // Pas de grammaire sur Android Chrome (bug connu)
      try {
        recognition.grammars = undefined;
      } catch (e) {}
      
    } else if (isMobile) {
      console.log('üì± Configuration mobile g√©n√©rique');
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'fr-FR';
      recognition.maxAlternatives = 3;
    } else {
      // Configuration desktop
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'fr-FR';
      recognition.maxAlternatives = 1;
    }

    recognition.onstart = () => {
      mobileLog.info('üéôÔ∏è Reconnaissance vocale d√©marr√©e');
      console.log('üéôÔ∏è Reconnaissance vocale d√©marr√©e');
      setIsListening(true);
    };

    // AJOUT DE TOUS LES √âV√âNEMENTS AUDIO POUR DIAGNOSTIC
    recognition.onaudiostart = () => {
      mobileLog.info('üéß onaudiostart - Audio captur√© par Speech Recognition');
    };

    recognition.onaudioend = () => {
      mobileLog.info('üéß onaudioend - Fin audio Speech Recognition');
    };

    recognition.onsoundstart = () => {
      mobileLog.info('üîä onsoundstart - Son d√©tect√© !');
    };

    recognition.onsoundend = () => {
      mobileLog.info('üîá onsoundend - Fin de son');
    };

    recognition.onspeechstart = () => {
      mobileLog.info('üé§ onspeechstart - PAROLE D√âTECT√âE !');
    };

    recognition.onspeechend = () => {
      mobileLog.info('üîá onspeechend - Fin de parole');
    };

    recognition.onnomatch = () => {
      mobileLog.warn('‚ùì onnomatch - Aucune correspondance trouv√©e');
    };

    recognition.onresult = (event: any) => {
      mobileLog.info('üéØ Speech onresult d√©clench√©, resultats: ' + event.results.length);
      console.log('üéôÔ∏è R√©sultat reconnaissance:', event.results);
      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        
        mobileLog.info(`üìù R√©sultat ${i}: "${transcript}" (final: ${event.results[i].isFinal})`);
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      setCurrentTranscript(interimTranscript);

      if (finalTranscript) {
        const cleanPhrase = finalTranscript.trim();
        mobileLog.info('‚úÖ Phrase captur√©e: "' + cleanPhrase + '"');
        mobileLog.info('üìä D√©tails: ' + cleanPhrase.length + ' chars, ' + cleanPhrase.split(' ').length + ' mots');
        
        console.log('‚úÖ Phrase captur√©e:', cleanPhrase);
        console.log('üìä D√©tail phrase:', {
          length: cleanPhrase.length,
          words: cleanPhrase.split(' ').length,
          timestamp: new Date().toLocaleTimeString()
        });
        setCapturedPhrases(prev => {
          const newPhrases = [...prev, cleanPhrase];
          mobileLog.info('üìù Total phrases maintenant: ' + newPhrases.length);
          console.log('üìù Total phrases maintenant:', newPhrases.length);
          console.log('üìã Liste compl√®te:', newPhrases);
          return newPhrases;
        });
        setCurrentTranscript('');
        
        // Red√©marrer la reconnaissance sur mobile si elle s'arr√™te
        if (isMobile && isRecording) {
          setTimeout(() => {
            if (recognitionRef.current && isRecording) {
              try {
                recognitionRef.current.start();
              } catch (err) {
                console.log('Reconnaissance d√©j√† active');
              }
            }
          }, 100);
        }
      }
    };

    recognition.onerror = (event: any) => {
      mobileLog.error('‚ùå Erreur reconnaissance vocale: ' + event.error);
      console.error('Erreur reconnaissance vocale:', event.error);
      
      if (event.error === 'not-allowed') {
        mobileLog.error('üö´ Permission microphone refus√©e');
        setError('Permission microphone refus√©e pour la reconnaissance vocale');
      } else if (event.error === 'network') {
        console.warn('Erreur r√©seau - red√©marrage reconnaissance...');
        // Red√©marrer automatiquement sur erreur r√©seau
        if (isMobile && isRecording) {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
            } catch (err) {
              console.log('Impossible de red√©marrer apr√®s erreur r√©seau');
            }
          }, 1000);
        }
      } else if (event.error === 'service-not-allowed') {
        console.warn('Service non autoris√© sur ce navigateur');
        setError('Service de reconnaissance vocale non autoris√© sur ce navigateur');
      } else if (event.error !== 'no-speech' && event.error !== 'aborted') {
        console.warn(`Erreur reconnaissance vocale: ${event.error}`);
        
        // Sur mobile, red√©marrer automatiquement pour la plupart des erreurs
        if (isMobile && isRecording && event.error !== 'audio-capture') {
          setTimeout(() => {
            try {
              recognitionRef.current?.start();
            } catch (err) {
              console.log('Impossible de red√©marrer apr√®s erreur');
            }
          }, 2000);
        }
      }
    };

    recognition.onend = () => {
      mobileLog.info('üîö Reconnaissance vocale termin√©e');
      console.log('üéôÔ∏è Reconnaissance vocale termin√©e');
      setIsListening(false);
      
      // Red√©marrage OBLIGATOIRE sur Android Chrome (se d√©connecte souvent)
      if (isAndroidChrome && isRecording) {
        mobileLog.info('ü§ñ Red√©marrage automatique Android Chrome...');
        console.log('ü§ñ Red√©marrage Android Chrome...');
        setTimeout(() => {
          if (recognitionRef.current && isRecording) {
            try {
              mobileLog.info('üîÑ Tentative red√©marrage...');
              recognitionRef.current.start();
              mobileLog.info('‚úÖ Red√©marrage Android Chrome r√©ussi');
              console.log('‚úÖ Red√©marrage Android Chrome r√©ussi');
            } catch (err) {
              mobileLog.error('‚ùå √âchec red√©marrage: ' + err);
              console.log('‚ùå √âchec red√©marrage Android Chrome:', err);
              // Forcer une phrase si √ßa ne marche pas
              setTimeout(() => {
                if (capturedPhrases.length === 0) {
                  setCapturedPhrases(prev => [...prev, "Reconnaissance Android difficile"]);
                }
              }, 1000);
            }
          }
        }, 500); // D√©lai rapide pour red√©marrage imm√©diat
        
      } else if (isMobile && isRecording) {
        // Autres mobiles
        setTimeout(() => {
          if (recognitionRef.current && isRecording) {
            try {
              recognitionRef.current.start();
            } catch (err) {
              console.log('Impossible de red√©marrer la reconnaissance');
            }
          }
        }, 500);
      }
    };

    recognitionRef.current = recognition;
    return true;
  };

  // D√âMARRER l'enregistrement (UN SEUL BOUTON!)
  const startRecording = async () => {
    // 0. R√âINITIALISER les phrases AU D√âBUT (pas √† la fin)
    setCapturedPhrases([]);
    mobileLog.info('üóëÔ∏è Phrases r√©initialis√©es au D√âBUT');
    
    // 1. V√©rifier les permissions
    const hasPerms = hasPermissions || await requestPermissions();
    if (!hasPerms) return;

    // 2. NOUVELLE STRAT√âGIE : Enregistrement simple + Whisper API apr√®s
    mobileLog.info('üéôÔ∏è NOUVELLE STRAT√âGIE : Enregistrement simple + Whisper API apr√®s');
    mobileLog.info('‚úÖ Pas de conflit Speech Recognition ‚Üí UX fluide');

    try {
      // 3. D√©marrage enregistrement vid√©o + audio (UX simple)
      mobileLog.info('üé¨ D√©marrage enregistrement normal (simple)');
      chunksRef.current = [];
      audioChunksRef.current = [];
      
      // VID√âO : Stream vid√©o seule (sans audio pour √©viter les conflits)
      const videoStream = new MediaStream();
      streamRef.current!.getVideoTracks().forEach(track => {
        videoStream.addTrack(track);
      });
      
      // D√©tection du format vid√©o support√©
      let videoMimeType = 'video/webm;codecs=vp9';
      if (!MediaRecorder.isTypeSupported(videoMimeType)) {
        videoMimeType = 'video/webm;codecs=vp8';
      }
      if (!MediaRecorder.isTypeSupported(videoMimeType)) {
        videoMimeType = 'video/webm';
      }
      
      const videoRecorder = new MediaRecorder(videoStream, {
        mimeType: videoMimeType || undefined,
        videoBitsPerSecond: 1000000 // 1Mbps vid√©o
      });

      videoRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      // AUDIO : Stream audio seule
      const audioStream = new MediaStream();
      streamRef.current!.getAudioTracks().forEach(track => {
        audioStream.addTrack(track);
      });
      
      // D√©tection du format audio support√©
      let audioMimeType = 'audio/webm;codecs=opus';
      if (!MediaRecorder.isTypeSupported(audioMimeType)) {
        audioMimeType = 'audio/webm';
      }
      if (!MediaRecorder.isTypeSupported(audioMimeType)) {
        audioMimeType = 'audio/wav';
      }
      
      const audioRecorder = new MediaRecorder(audioStream, {
        mimeType: audioMimeType || undefined,
        audioBitsPerSecond: 128000 // 128kbps audio
      });

      audioRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      // Callback quand les deux enregistrements sont termin√©s
      let videoFinished = false;
      let audioFinished = false;
      
      const checkBothFinished = async () => {
        if (videoFinished && audioFinished) {
          const videoBlob = new Blob(chunksRef.current, { type: videoMimeType });
          
          // Stocker l'audio s√©par√©
          setAudioChunks(audioChunksRef.current);
          setHasAudioInVideo(audioChunksRef.current.length > 0);
          
          // NOUVELLE STRAT√âGIE : Transcrire avec Whisper API
          let finalPhrases = capturedPhrases;
          console.log('üîç DEBUG finalPhrases avant Whisper:', finalPhrases);
          
          // Si on a de l'audio, transcrire avec Whisper
          if (audioChunksRef.current.length > 0) {
            try {
              mobileLog.info('üéôÔ∏è D√©but transcription Whisper API...');
              
              // Cr√©er le blob audio
              const debugAudioBlob = new Blob(audioChunksRef.current, {
                type: 'audio/webm;codecs=opus'
              });
              
              // Appeler Whisper API via service s√©curis√©
              const aiService = new (await import('@/services/aiService')).AIService();
              const transcription = await aiService.transcribeAudio(debugAudioBlob);
              
              if (transcription) {
                // S√©parer les phrases par des points ou des silences
                const phrases = transcription.split(/[.!?]+/).filter(p => p.trim().length > 0);
                finalPhrases = phrases.map(p => p.trim());
                mobileLog.info('‚úÖ Transcription Whisper r√©ussie: ' + phrases.length + ' phrases');
                mobileLog.info('üìù Phrases: ' + finalPhrases.join(' | '));
              } else {
                mobileLog.warn('‚ö†Ô∏è Transcription Whisper vide');
                finalPhrases = ['Transcription vide - parlez plus fort'];
              }
              
            } catch (error) {
              mobileLog.error('‚ùå Erreur Whisper: ' + error);
              console.error('Erreur Whisper:', error);
              finalPhrases = ['Erreur transcription - mode visuel seulement'];
            }
          }
          
          // D√âSACTIV√â TEMPORAIREMENT
          // if (finalPhrases.length === 0 && audioChunksRef.current.length > 0) {
          //   finalPhrases = [
          //     "Objets d√©tect√©s dans l'enregistrement audio",
          //     "Analyse IA n√©cessaire pour identifier les objets"
          //   ];
          //   setCapturedPhrases(finalPhrases);
          // } else if (finalPhrases.length === 0) {
          //   // M√™me sans audio, permettre de continuer avec la vid√©o
          //   finalPhrases = [
          //     "Analyse par vid√©o seulement",
          //     "Objets d√©tect√©s visuellement"
          //   ];
          //   setCapturedPhrases(finalPhrases);
          // }
          
          console.log('üì§ Enregistrement termin√©:', {
            video: chunksRef.current.length + ' chunks',
            audio: audioChunksRef.current.length + ' chunks', 
            phrases: finalPhrases.length
          });
          
          // Cr√©er un blob audio depuis les chunks audio pour le debug
          let debugAudioBlob: Blob | undefined;
          if (audioChunksRef.current.length > 0) {
            debugAudioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
            console.log('üéß Audio blob cr√©√©:', debugAudioBlob.size, 'bytes');
          } else {
            console.warn('‚ö†Ô∏è Aucun chunk audio trouv√©');
          }

          // Mettre √† jour l'√©tat avec les phrases Whisper
          setCapturedPhrases(finalPhrases);
          
          onRecordingComplete(videoBlob, finalPhrases, debugAudioBlob);
        }
      };

      videoRecorder.onstop = () => {
        videoFinished = true;
        checkBothFinished();
      };

      audioRecorder.onstop = () => {
        audioFinished = true;
        checkBothFinished();
      };

      mediaRecorderRef.current = videoRecorder;
      audioRecorderRef.current = audioRecorder;
      
      // D√©marrer les deux enregistrements
      videoRecorder.start(1000);
      audioRecorder.start(1000);

      // 4. PAS de Speech Recognition ‚Üí WHISPER API utilis√© apr√®s l'enregistrement
      mobileLog.info('‚úÖ Speech Recognition d√©sactiv√© ‚Üí Whisper API sera utilis√© apr√®s');
      
      // Interface temporaire : indiquer qu'on utilise Whisper
      setTimeout(() => {
        setCapturedPhrases(prev => [...prev, "üéôÔ∏è Audio sera transcrit par Whisper API..."]);
      }, 1000);

      // 5. D√©marrer le timer
      setRecordingTime(0);
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);

      setIsRecording(true);
      // setCapturedPhrases([]); // SUPPRIM√â ! Cela effa√ßait les phrases captur√©es pendant le d√©lai !
      setError(null);
      
      mobileLog.info('üìù Phrases actuelles conserv√©es: ' + capturedPhrases.length);
      
    } catch (err) {
      console.error('Erreur d√©marrage enregistrement:', err);
      setError('Impossible de d√©marrer l\'enregistrement.');
    }
  };

  // ARR√äTER l'enregistrement
  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
    }
    
    if (audioRecorderRef.current && isRecording) {
      audioRecorderRef.current.stop();
    }
    
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }

    setIsRecording(false);
    setIsListening(false);
  };

  // Format du temps
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <div className="bg-white rounded-xl border-2 border-purple-200 shadow-lg overflow-hidden">
      {/* Header mobile-optimized */}
      <div className="bg-gradient-to-r from-purple-600 to-pink-600 text-white p-4">
        <h2 className="text-xl font-bold text-center">üì± DodoLens Mobile</h2>
        <p className="text-purple-100 text-sm text-center mt-1">
          UN SEUL TAP : Vid√©o + Audio simultan√©s
        </p>
      </div>

      {/* Vid√©o preview */}
      <div className="relative">
        <video
          ref={videoRef}
          autoPlay
          playsInline
          muted
          className="w-full h-64 sm:h-80 object-cover bg-gray-900"
        />
        
        {/* Overlay d'informations */}
        {isRecording && (
          <div className="absolute top-4 left-4 right-4">
            <div className="bg-black bg-opacity-70 text-white rounded-lg p-3">
              <div className="flex items-center justify-between mb-2">
                <div className="flex items-center space-x-2">
                  <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
                  <span className="font-bold">REC</span>
                </div>
                <span className="font-mono text-lg">{formatTime(recordingTime)}</span>
              </div>
              
              <div className="flex items-center justify-between text-sm">
                <div className="flex items-center space-x-2">
                  <span>üéôÔ∏è</span>
                  <span className={isListening ? 'text-green-400' : 'text-yellow-400'}>
                    {isListening ? 'Reconnaissance ACTIVE' : 'En attente...'}
                  </span>
                  {!isListening && isRecording && (
                    <span className="text-xs text-yellow-300 animate-pulse">
                      Essai de connexion...
                    </span>
                  )}
                </div>
                
                {/* Indicateur niveau audio */}
                <div className="flex items-center space-x-1">
                  <span className="text-xs">Vol:</span>
                  <div className="w-12 h-2 bg-gray-600 rounded-full overflow-hidden">
                    <div 
                      className={`h-full transition-all duration-100 ${
                        audioLevel > 10 ? 'bg-green-400' : 'bg-red-400'
                      }`}
                      style={{ width: `${Math.min(100, audioLevel * 2)}%` }}
                    ></div>
                  </div>
                  <span className="text-xs">{audioLevel}</span>
                </div>
              </div>
            </div>
          </div>
        )}

        {/* Transcript en temps r√©el */}
        {(currentTranscript || capturedPhrases.length > 0) && (
          <div className="absolute bottom-4 left-4 right-4">
            <div className="bg-white bg-opacity-90 rounded-lg p-3 max-h-24 overflow-y-auto">
              {capturedPhrases.length > 0 && (
                <div className="text-xs text-green-600 mb-1">
                  ‚úÖ {capturedPhrases.length} phrase(s) captur√©e(s)
                </div>
              )}
              {currentTranscript && (
                <div className="text-sm text-gray-800">
                  "{currentTranscript}"
                </div>
              )}
            </div>
          </div>
        )}
      </div>

      {/* Contr√¥les */}
      <div className="p-6 space-y-4">
        {error && (
          <div className="bg-red-50 border border-red-200 rounded-lg p-3">
            <div className="text-red-800 text-sm">{error}</div>
          </div>
        )}

        {hasPermissions === null && (
          <div className="bg-blue-50 border border-blue-200 rounded-lg p-4 text-center">
            <div className="text-blue-800 font-medium mb-2">üì± Pr√™t pour le test mobile ?</div>
            <div className="text-blue-600 text-sm mb-4">
              Nous allons demander l'acc√®s √† votre cam√©ra et microphone
            </div>
            <button
              onClick={requestPermissions}
              className="bg-blue-600 text-white px-6 py-3 rounded-lg font-medium hover:bg-blue-700 transition-colors"
            >
              üîì Autoriser l'acc√®s
            </button>
          </div>
        )}

        {hasPermissions && !isRecording && (
          <div className="text-center space-y-4">
            <div className="text-green-600 text-sm">‚úÖ Permissions accord√©es</div>
            
            {/* Diagnostic audio/vid√©o */}
            <div className="bg-gray-50 border rounded-lg p-3 text-left">
              <div className="text-xs font-medium text-gray-700 mb-2">üîß Diagnostic technique :</div>
              <div className="space-y-1 text-xs text-gray-600">
                <div>üì± Mobile d√©tect√©: {isMobile ? 'Oui' : 'Non'}</div>
                <div>üéôÔ∏è Niveau audio: {audioLevel > 5 ? '‚úÖ Actif' : '‚ö†Ô∏è Silencieux'} ({audioLevel})</div>
                <div>üé¨ Formats support√©s: {supportedMimeTypes.length}</div>
                {supportedMimeTypes.length > 0 && (
                  <div className="text-xs">
                    ‚Ä¢ {supportedMimeTypes[0]}
                  </div>
                )}
                <div>üîä Speech Recognition: {('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) ? '‚úÖ' : '‚ùå'}</div>
                {audioChunks.length > 0 && (
                  <div className="mt-2 p-2 bg-blue-50 rounded">
                    <div className="text-xs font-medium text-blue-700">üéµ Audio s√©par√© disponible:</div>
                    <button
                      onClick={() => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        const url = URL.createObjectURL(audioBlob);
                        const a = document.createElement('a');
                        a.href = url;
                        a.download = `dodo-lens-audio-${Date.now()}.webm`;
                        a.click();
                      }}
                      className="text-xs bg-blue-600 text-white px-2 py-1 rounded mt-1"
                    >
                      üì• T√©l√©charger Audio
                    </button>
                  </div>
                )}
              </div>
            </div>
            
            {/* LE BOUTON R√âVOLUTIONNAIRE */}
            <button
              onClick={startRecording}
              className="w-full bg-gradient-to-r from-purple-600 to-pink-600 text-white py-6 rounded-xl font-bold text-xl shadow-lg hover:shadow-xl transform hover:scale-105 transition-all duration-200"
            >
              üé¨ D√âMARRER<br/>
              <span className="text-lg font-normal">Vid√©o + Audio simultan√©s</span>
            </button>
            
            <div className="text-xs text-gray-500">
              Un seul tap pour tout enregistrer !
            </div>
          </div>
        )}

        {isRecording && (
          <div className="text-center space-y-4">
            <div className="text-purple-600 font-medium">
              üé¨ Enregistrement en cours...
            </div>
            
            <button
              onClick={stopRecording}
              className="w-full bg-red-600 text-white py-6 rounded-xl font-bold text-xl shadow-lg hover:shadow-xl transform hover:scale-105 transition-all duration-200"
            >
              ‚èπÔ∏è ARR√äTER<br/>
              <span className="text-lg font-normal">Terminer l'enregistrement</span>
            </button>
            
            <div className="space-y-3">
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div className="bg-purple-50 p-3 rounded-lg">
                  <div className="font-medium text-purple-600">üìπ Vid√©o</div>
                  <div className="text-purple-500">{formatTime(recordingTime)}</div>
                </div>
                <div className="bg-green-50 p-3 rounded-lg">
                  <div className="font-medium text-green-600">üéôÔ∏è Audio</div>
                  <div className="text-green-500">{capturedPhrases.length} phrases</div>
                  <div className="text-xs text-green-600">
                    {hasAudioInVideo ? '‚úÖ Audio captur√©' : '‚ö†Ô∏è Audio s√©par√©'}
                  </div>
                </div>
              </div>
              
              {/* Boutons manuels si la reconnaissance vocale ne marche pas */}
              {!isListening && isRecording && (
                <div className="bg-yellow-50 border border-yellow-200 rounded-lg p-3">
                  <div className="text-xs font-medium text-yellow-800 mb-2">
                    üó£Ô∏è Reconnaissance vocale inactive - Ajout manuel :
                  </div>
                  <div className="grid grid-cols-2 gap-2">
                    <button
                      onClick={() => setCapturedPhrases(prev => [...prev, "Je prends ce meuble"])}
                      className="text-xs bg-green-600 text-white px-2 py-1 rounded"
                    >
                      ‚úÖ Je prends
                    </button>
                    <button
                      onClick={() => setCapturedPhrases(prev => [...prev, "J'ignore cet objet"])}
                      className="text-xs bg-red-600 text-white px-2 py-1 rounded"
                    >
                      ‚ùå J'ignore
                    </button>
                  </div>
                </div>
              )}
            </div>
          </div>
        )}
      </div>
    </div>
  );
};
